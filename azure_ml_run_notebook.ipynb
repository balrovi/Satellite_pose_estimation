{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02d191bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Azure ML SDK Version:  1.33.0\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "import azureml\n",
    "from azureml.core.model import Model, InferenceConfig\n",
    "from azureml.core import Workspace, Datastore, Experiment, Run, Environment, ScriptRunConfig\n",
    "\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute, AksCompute, ComputeTarget\n",
    "from azureml.train.dnn import PyTorch\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "from azureml.core.webservice import Webservice, AksWebservice, AciWebservice\n",
    "from azureml.core.dataset import Dataset\n",
    "from azureml.core.resource_configuration import ResourceConfiguration\n",
    "from azureml.core.conda_dependencies import CondaDependencies \n",
    "\n",
    "# check core SDK version number\n",
    "print(\"Azure ML SDK Version: \", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159b71b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FIRST TIME SETUP\n",
    "#Change the path to your local SPEED data directory after download from https://kelvins.esa.int/satellite-pose-estimation-challenge/data/\n",
    "DATA_PATH= \"/home/salem/Documents/DLR/Challenge/speed\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7473e863",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "#FIRST TIME SETUP : define these two variables after azure account creation\n",
    "tenant_id = \"f77a90dc-721b-4ae2-8371-7629ac13542d\" #<your_tenant_id>\n",
    "subscription_id= \"e50a493c-c8fe-4da7-8361-e99f6349f177\" #<your_tenant_id>\n",
    "\n",
    "\n",
    "#reuse the commented lines if you want to switch to another subscription\n",
    "# from azureml.core.authentication import InteractiveLoginAuthentication\n",
    "# forced_interactive_auth = InteractiveLoginAuthentication(tenant_id=tenant_id, force=True)\n",
    "\n",
    "\n",
    "# Create the workspace using the specified parameters\n",
    "ws = Workspace.create(name = \"VisionLab\",\n",
    "                      subscription_id = subscription_id\n",
    "\n",
    "                      resource_group = \"satellite_pose_estimation\", \n",
    "                      location = \"eastus\",\n",
    "                      create_resource_group = True,\n",
    "                      exist_ok = True)\n",
    "ws.get_details()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1dcc86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep = '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6020e56b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing compute target\n"
     ]
    }
   ],
   "source": [
    "cluster_name = \"OptimusPrime\" \n",
    "\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print('Found existing compute target')\n",
    "except :\n",
    "    print('Creating a new compute target...')\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_NC6',\n",
    "                                                           min_nodes = 0, \n",
    "                                                           max_nodes=5)\n",
    "\n",
    "    compute_target = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "\n",
    "    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c56d8cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datastore: workspaceblobstore\n",
      "Experiment: Setup_trial\n"
     ]
    }
   ],
   "source": [
    "# Connect to the datastore for the training images\n",
    "ds = Datastore.get_default(ws)\n",
    "print(\"Datastore:\",ds.name)\n",
    "\n",
    "#FIRST TIME SETUP : \n",
    "# Do the following the first time you are setting the dataset on azure storage\n",
    "# speed_data = ds.upload(src_dir = DATA_PATH ,target_path = \"speed\", show_progress = True)\n",
    "\n",
    "# Connect to the experiment\n",
    "exp = Experiment(workspace=ws, name='Setup_trial')\n",
    "print(\"Experiment:\",exp.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "914a7260",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Dataset\n",
    "\n",
    "# create the dataset object\n",
    "dataset = Dataset.File.from_files(path=(ds, '/speed'))\n",
    "\n",
    "# register the dataset for future use\n",
    "dataset = dataset.register(workspace=ws,\n",
    "                           name='speed_dataset',\n",
    "                           description=\"Satellite images in train, test, real test folders and their pose labels in json files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5fe005dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an enviornment\n",
    "env = Environment(name='sat_pose_estimation')\n",
    "\n",
    "# define packages for image\n",
    "cd = CondaDependencies.create(pip_packages=['azureml-dataset-runtime[pandas,fuse]', \n",
    "                                            'azureml-defaults',\n",
    "                                            \"torch\",\n",
    "                                            \"torchvision\",\n",
    "                                            \"pytorch-lightning\",\n",
    "                                            \"numpy\",\n",
    "                                            \"matplotlib\",\n",
    "                                            'pillow'],\n",
    "                             conda_packages=['SciPy'])\n",
    "\n",
    "env.python.conda_dependencies = cd\n",
    "\n",
    "# Specify a docker image to use.\n",
    "env.docker.base_image = (\n",
    "    \"mcr.microsoft.com/azureml/openmpi4.1.0-cuda11.0.3-cudnn8-ubuntu18.04\"\n",
    ")\n",
    "\n",
    "\n",
    "# Register environment to re-use later\n",
    "env = env.register(workspace = ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80d66b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a directory for the training script\n",
    "os.makedirs('train_script', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae6767ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting train_script/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train_script/train.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "\n",
    "import torch \n",
    "from torch import nn \n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split \n",
    "from torchvision import transforms, models\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from utils import PyTorchSatellitePoseEstimationDataset\n",
    "from submission import SubmissionWriter\n",
    "from azureml_env_adapter import set_environment_variables\n",
    "from pytorch_lightning.plugins import DDPPlugin\n",
    "\n",
    "# output will be logged, separate output from previous log entries.\n",
    "print('-'*100)\n",
    "\n",
    "\n",
    "def add_model_specific_args(parent_parser):\n",
    "    parser = argparse.ArgumentParser(parents=[parent_parser], add_help=False)\n",
    "    parser.add_argument('--num_workers', type=int, default=8)\n",
    "    parser.add_argument('--batch_size', type=int, default=32)\n",
    "    return parser\n",
    "\n",
    "class SatellitePoseEstimationModel(pl.LightningModule):\n",
    "    def __init__(self, submission = None) :\n",
    "        super().__init__() \n",
    "        initialized_model = models.resnet18(pretrained=True)\n",
    "        num_ftrs = initialized_model.fc.in_features\n",
    "        initialized_model.fc = torch.nn.Linear(num_ftrs, 7)\n",
    "        self.model = initialized_model\n",
    "        self.submission = submission\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.model(x)\n",
    "        \n",
    "    def training_step(self,batch ,batch_idx):\n",
    "        x,y = batch \n",
    "        y_hat = self.model(x)\n",
    "        loss = F.mse_loss(y_hat.float(),y.float())\n",
    "        self.log('step', self.trainer.current_epoch+1)\n",
    "        self.log('losses', {'train': loss})\n",
    "\n",
    "#         self.log(\"train_loss\",loss)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x,y = batch \n",
    "        y_hat = self.model(x)\n",
    "        loss = F.mse_loss(y_hat.float(),y.float())\n",
    "        self.log('step', self.trainer.current_epoch+1)\n",
    "        self.log('losses', {'valid': loss})\n",
    "#         self.log(\"val_loss\", loss) #, on_epoch=True)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(),lr = 0.001)\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        inputs, filenames = batch \n",
    "        outputs = self.model(inputs)\n",
    "        \n",
    "        q_batch = outputs[:, :4].cpu().numpy()\n",
    "        r_batch = outputs[:, -3:].cpu().numpy()\n",
    "    \n",
    "        for filename, q, r in zip(filenames, q_batch, r_batch):\n",
    "            self.submission.append_test(filename, q, r)  \n",
    "\n",
    "class DataModule(pl.LightningDataModule) : \n",
    "    def __init__(self, batch_size = 32, num_workers = 8, speed_root=''):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size \n",
    "        #num_workers = 4*gpu_num\n",
    "        self.num_workers = num_workers \n",
    "        self.speed_root = speed_root\n",
    "\n",
    "    def setup(self, stage = None):\n",
    "        #Transforms \n",
    "        data_transforms = transforms.Compose([transforms.Resize((224, 224)),\n",
    "                                              transforms.ToTensor(),\n",
    "                                              transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                                                                   [0.229, 0.224, 0.225])])\n",
    "        full_dataset = PyTorchSatellitePoseEstimationDataset('train', self.speed_root, data_transforms)\n",
    "        if stage == \"fit\" or stage is None:\n",
    "            self.train_dataset, self.val_dataset = torch.utils.data.random_split(full_dataset, \n",
    "                                                                   [int(len(full_dataset) * .8),\n",
    "                                                                    int(len(full_dataset) * .2)])\n",
    "        if stage == \"test\" or stage is None:\n",
    "            self.test_dataset = PyTorchSatellitePoseEstimationDataset('test', self.speed_root, data_transforms)\n",
    "            \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=self.num_workers) \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=True, num_workers=self.num_workers)\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size = self.batch_size, num_workers = self.num_workers)    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    \n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--data-path', type=str, \n",
    "                        dest='data_path', \n",
    "                        default='data', \n",
    "                        help='data folder mounting point')\n",
    "    \n",
    "    parser.add_argument(\"--logdir\", default=\"./logs\", type=str)\n",
    "    \n",
    "    parser = add_model_specific_args(parser)\n",
    " \n",
    "    parser = pl.Trainer.add_argparse_args(parser)\n",
    "    \n",
    "    # parse the parameters passed to the this script\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    trial_name = f\"first_model_{args.max_epochs}epochs\"\n",
    "    # set azureml env vars for multi-node ddp\n",
    "    set_environment_variables()\n",
    "    \n",
    "#     MySubmission = SubmissionWriter()\n",
    "    \n",
    "    model = SatellitePoseEstimationModel()\n",
    "    dm = DataModule(batch_size = args.batch_size, num_workers = args.num_workers, speed_root = args.data_path )\n",
    "    \n",
    "    tb_logger = TensorBoardLogger(args.logdir, name = trial_name)\n",
    "    \n",
    "    \n",
    "    # ------------\n",
    "    # training\n",
    "    # ------------\n",
    "    trainer = pl.Trainer.from_argparse_args(args, logger=tb_logger, plugins=DDPPlugin(find_unused_parameters=False))    \n",
    "    try : \n",
    "        trainer.fit(model, dm) \n",
    "    except : \n",
    "        print(\"ERROR : The model stoped training !\")\n",
    "    finally : \n",
    "        print('Saving model...')\n",
    "        trainer.save_checkpoint(f\"outputs/{trial_name}.ckpt\")\n",
    "#         trainer.test(model = model, datamodule = dm)\n",
    "#         print(MySubmission.test_results)\n",
    "#         MySubmission.export(out_dir=\"./outputs\", suffix= trial_name)\n",
    "    print('Done!')\n",
    "    print('-'*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "379ee8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.runconfig import MpiConfiguration\n",
    "from azureml.core.runconfig import DockerConfiguration\n",
    "# setup the run details\n",
    "num_nodes = 1 \n",
    "\n",
    "\n",
    "src = ScriptRunConfig(source_directory='train_script',\n",
    "                      script='train.py',\n",
    "                      arguments=['--data-path', dataset.as_mount(),\n",
    "                                 '--logdir','./logs',\n",
    "                                 '--num_workers',4,\n",
    "                                 '--batch_size', 32,\n",
    "                                 '--gpus',-1,\n",
    "                                 '--max_epochs',10,\n",
    "                                 '--accelerator','ddp',\n",
    "                                 '--num_nodes', num_nodes],\n",
    "                      compute_target=compute_target, #\"local\" #to run locally\n",
    "                      \n",
    "                      distributed_job_config = MpiConfiguration(node_count=num_nodes),\n",
    "                      \n",
    "                      docker_runtime_config = DockerConfiguration(use_docker=True, \n",
    "                                                                  shared_volumes=True, \n",
    "                                                                  arguments=[\"--ipc\",\"host\"], \n",
    "                                                                  shm_size='2g'),\n",
    "                      environment=env)\n",
    "\n",
    "# Submit the model to azure!\n",
    "run = exp.submit(config=src)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33cef308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:6006/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'http://localhost:6006/'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azureml.tensorboard import Tensorboard\n",
    "\n",
    "tb = Tensorboard([run])\n",
    "\n",
    "# If successful, start() returns a string with the URI of the instance.\n",
    "tb.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5025fded",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_22851/2181986964.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# After your job completes, be sure to stop() the streaming otherwise it will continue to run.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'tb' is not defined"
     ]
    }
   ],
   "source": [
    "# After your job completes, be sure to stop() the streaming otherwise it will continue to run. \n",
    "tb.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f97df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO (first priority) : Import a run and get the tensorboard plot of losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "123a3edb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run(Experiment: Setup_trial,\n",
      "Id: Setup_trial_1630167182_fd1371b0,\n",
      "Type: azureml.scriptrun,\n",
      "Status: Completed)\n",
      "Run(Experiment: Setup_trial,\n",
      "Id: Setup_trial_1630165769_c3fde39b,\n",
      "Type: azureml.scriptrun,\n",
      "Status: Completed)\n",
      "Run(Experiment: Setup_trial,\n",
      "Id: Setup_trial_1630164941_43727c92,\n",
      "Type: azureml.scriptrun,\n",
      "Status: Completed)\n",
      "Run(Experiment: Setup_trial,\n",
      "Id: Setup_trial_1630161254_e2a88ad8,\n",
      "Type: azureml.scriptrun,\n",
      "Status: Completed)\n",
      "Run(Experiment: Setup_trial,\n",
      "Id: Setup_trial_1630159084_c9ed9394,\n",
      "Type: azureml.scriptrun,\n",
      "Status: Completed)\n",
      "Run(Experiment: Setup_trial,\n",
      "Id: Setup_trial_1630157137_efdeac6f,\n",
      "Type: azureml.scriptrun,\n",
      "Status: Completed)\n",
      "Run(Experiment: Setup_trial,\n",
      "Id: Setup_trial_1630156219_64e4117f,\n",
      "Type: azureml.scriptrun,\n",
      "Status: Failed)\n",
      "Run(Experiment: Setup_trial,\n",
      "Id: Setup_trial_1630149734_802f2495,\n",
      "Type: azureml.scriptrun,\n",
      "Status: Failed)\n",
      "Run(Experiment: Setup_trial,\n",
      "Id: Setup_trial_1630070102_5d32daf2,\n",
      "Type: azureml.scriptrun,\n",
      "Status: Failed)\n",
      "Run(Experiment: Setup_trial,\n",
      "Id: Setup_trial_1630067580_8fdeede9,\n",
      "Type: azureml.scriptrun,\n",
      "Status: Failed)\n",
      "Run(Experiment: Setup_trial,\n",
      "Id: Setup_trial_1629939052_704cd684,\n",
      "Type: azureml.scriptrun,\n",
      "Status: Completed)\n",
      "Run(Experiment: Setup_trial,\n",
      "Id: Setup_trial_1629914761_6ed7ff88,\n",
      "Type: azureml.scriptrun,\n",
      "Status: Completed)\n",
      "Run(Experiment: Setup_trial,\n",
      "Id: Setup_trial_1629899850_372c1503,\n",
      "Type: azureml.scriptrun,\n",
      "Status: Completed)\n",
      "Run(Experiment: Setup_trial,\n",
      "Id: Setup_trial_1629898222_65a64c4b,\n",
      "Type: azureml.scriptrun,\n",
      "Status: Completed)\n",
      "Run(Experiment: Setup_trial,\n",
      "Id: c4c2c6f9-8a44-4e0c-bf73-a0ffe0444f3a,\n",
      "Type: None,\n",
      "Status: Completed)\n",
      "Run(Experiment: Setup_trial,\n",
      "Id: Setup_trial_1629895138_dc42586b,\n",
      "Type: azureml.scriptrun,\n",
      "Status: Completed)\n",
      "Run(Experiment: Setup_trial,\n",
      "Id: Setup_trial_1629891870_f8837879,\n",
      "Type: azureml.scriptrun,\n",
      "Status: Failed)\n",
      "Run(Experiment: Setup_trial,\n",
      "Id: Setup_trial_1629889947_332fcb94,\n",
      "Type: azureml.scriptrun,\n",
      "Status: Completed)\n",
      "Run(Experiment: Setup_trial,\n",
      "Id: Setup_trial_1629823357_d4ca9e15,\n",
      "Type: azureml.scriptrun,\n",
      "Status: Completed)\n"
     ]
    }
   ],
   "source": [
    "filtered_list_runs = Run.list(exp)\n",
    "for run_name in filtered_list_runs : \n",
    "    print(run_name)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "98a0b283c4823b0d3acec4e4403e279d8cf4c2a18d836c1d79eb158cb509fe5f"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
